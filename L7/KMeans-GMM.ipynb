{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS5340 Lecture 2:  k-Means, GMMs,and Expectation Maximization#\n",
    "\n",
    "\n",
    "Lecturer: Harold Soh (harold@comp.nus.edu.sg)\n",
    "\n",
    "Graduate TAs: Abdul Fatir Ansari and Chen Kaiqi (AY19/20)\n",
    "\n",
    "This notebook is a supplement to Lecture 7 of CS5340: Uncertainty Modeling in AI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from scipy.stats import multivariate_normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# computes the elucidean distances from each point X to each point in C\n",
    "def euclidean(X, c):\n",
    "    c = c.reshape(1, X.shape[1])\n",
    "    return np.linalg.norm(X - c, ord=2, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create some data!\n",
    "# you can play aorund with this!\n",
    "dataset = 'varied' # 'simple', 'varied' or 'anisotropic'\n",
    "n_samples = 1000\n",
    "\n",
    "# set random seed for consistency\n",
    "seed = 170\n",
    "np.random.seed(seed)\n",
    "\n",
    "if dataset == 'simple':\n",
    "    X, y = make_blobs(n_samples=n_samples, random_state=seed, centers=3)\n",
    "elif dataset == 'varied':\n",
    "    # blobs with varied variances\n",
    "    X,y = make_blobs(n_samples=n_samples,cluster_std=[1.0, 2.5, 0.5],random_state=seed)\n",
    "elif dataset == 'anisotropic':\n",
    "    X, y = make_blobs(n_samples=n_samples, random_state=seed, centers=3)\n",
    "    transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "    X = np.dot(X, transformation)\n",
    "else:\n",
    "    print('No such dataset! Defaulting to easy')\n",
    "    X, y = make_blobs(n_samples=n_samples, random_state=seed, centers=3)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=3)\n",
    "plt.title('Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KMeans\n",
    "\n",
    "* Set the number of clusters $k$ and randomly initialize the clusters centers $\\{\\mu_i\\}_{i=1}^k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed for consistency\n",
    "seed = 1\n",
    "np.random.seed(seed)\n",
    "\n",
    "k = 3  # Number of clusters\n",
    "d = X.shape[1]  # Dimensionality of data points\n",
    "labels = np.arange(k, dtype=np.uint)\n",
    "cx = np.random.uniform(low=X[:, 0].min(), high=X[:, 0].max(), size=k)[:, None]\n",
    "cy = np.random.uniform(low=X[:, 1].min(), high=X[:, 1].max(), size=k)[:, None]\n",
    "centers = np.hstack([cx, cy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment Step\n",
    "* Assign each datapoint to the nearest cluster center.\n",
    "\n",
    "### Update Step\n",
    "* Update each cluster center to the mean of the points assigned to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "itr = 0\n",
    "max_iter = 100\n",
    "plot_freq = 10\n",
    "\n",
    "while True and itr < max_iter:\n",
    "    \n",
    "    # Assignment step\n",
    "    distances = np.asarray([euclidean(X, c) for c in centers]) # k x n array of distances\n",
    "    assignments = np.argmin(distances, 0) # list of size 'n', cluster indices for each datapoint\n",
    "    \n",
    "    # Plot current assignments\n",
    "    if itr%plot_freq == 0:\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=assignments, s=3)\n",
    "        plt.scatter(centers[:, 0], centers[:, 1], marker='X', color='r')\n",
    "        plt.title('Step {:d}'.format(itr+1))\n",
    "        plt.show()\n",
    "    \n",
    "    # Update step\n",
    "    centers = np.asarray([np.mean(X[assignments == l], 0) for l in labels]) # k x d array of cluster centers\n",
    "    \n",
    "    # check converged\n",
    "    if itr > 0:\n",
    "        if (prev_assgn == assignments).all():\n",
    "            print(\"Converged at Step %d!\"%(itr))\n",
    "            break\n",
    "        \n",
    "    prev_assgn = assignments\n",
    "    \n",
    "    itr += 1\n",
    "    \n",
    "plt.scatter(X[:, 0], X[:, 1], c=assignments, s=3)\n",
    "plt.scatter(centers[:, 0], centers[:, 1], marker='X', color='r')\n",
    "plt.title('Step {:d}'.format(itr+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMM\n",
    "\n",
    "The likelihood of a GMM is given by\n",
    "$$\n",
    "p(x_1, \\dots, x_n|\\theta) = \\prod_{i=1}^n\\sum_{j=1}^k \\pi_k\\mathcal{N}(x_i;\\mu_j,\\Sigma_j)\n",
    "$$\n",
    "\n",
    "* Intialize $\\pi_k, \\mu_k, \\Sigma_k$.\n",
    "* **E-Step**: Evaluate responsibilities $\\gamma(z_{nk})$ where $\\gamma(z_{nk}) = p(z_{nk}=1|x_n)$.\n",
    "* **M-Step**: Assign new values to $\\pi_k, \\mu_k, \\Sigma_k$ by maximizing the log-likelihood w.r.t. each parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "k = 3\n",
    "pis = np.ones(k)/(k)\n",
    "rgb = np.array([np.array((1., 0., 0.)), np.array((0., 1., 0.)), np.array((0., 0., 1.)), np.array((0., 0., 0.))])\n",
    "cx = np.random.uniform(low=X[:, 0].min(), high=X[:, 0].max(), size=len(pis))[:, None]\n",
    "cy = np.random.uniform(low=X[:, 1].min(), high=X[:, 1].max(), size=len(pis))[:, None]\n",
    "mus = np.hstack([cx, cy])\n",
    "Sigmas = np.asarray([np.eye(X.shape[1]) for l in pis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Estep(pis, mus, Sigmas):\n",
    "    # get the responsibilities\n",
    "    pi_N = np.asarray([pis[i] * multivariate_normal.pdf(X, mus[i], Sigmas[i]) for i in range(k)]) # k x n\n",
    "    resp = pi_N / np.sum(pi_N, 0, keepdims=True) # k x n\n",
    "    return resp, pi_N\n",
    "\n",
    "def Mstep(resp, pis, mus, Sigmas):\n",
    "    mus = np.sum(resp[:, :, None] * X[None, :, :], axis=1)/np.sum(resp, axis=1, keepdims=True)\n",
    "    S = X[None, :, :, None] - mus[:, None, :, None] # k x n x d x 1\n",
    "    S_ST = np.matmul(S, np.transpose(S, axes=(0, 1, 3, 2))) # k x n x d x d\n",
    "    Sigmas = np.sum(S_ST * resp[:, :, None, None], axis=1)\n",
    "    Sigmas = Sigmas / (np.sum(resp, axis=1).reshape(k, 1, 1))\n",
    "    pis = np.sum(resp, axis=1) / np.sum(resp)\n",
    "    return pis, mus, Sigmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "prev_ll = -np.inf\n",
    "itr = 0\n",
    "max_itr = 1000\n",
    "plot_freq = 1\n",
    "while True and itr < max_itr:\n",
    "    # E-Step\n",
    "    resp, pi_N = Estep(pis, mus, Sigmas) \n",
    "    \n",
    "    # Compute log-likelihood and check for convergence\n",
    "    log_ll = np.log(1e-8 + np.sum(pi_N, 0)).sum()\n",
    "    if log_ll - prev_ll < 0.1:\n",
    "        print('Converged at step %d'%(itr))\n",
    "        break\n",
    "            \n",
    "    # Plot results\n",
    "    if itr % plot_freq == 0:\n",
    "        # Assign a color to each datapoint\n",
    "        colors = np.sum(resp[:, None, :] * rgb[:k, :, None], 0).T\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=colors, s=3)\n",
    "        plt.scatter(mus[:, 0], mus[:, 1], marker='X', edgecolors='k')\n",
    "        plt.title('Step %d : log-likelihood: %.2f'%(itr, log_ll))\n",
    "        plt.show()\n",
    "    \n",
    "    # M-Step\n",
    "    pis, mus, Sigmas = Mstep(resp, pis, mus, Sigmas)\n",
    "    \n",
    "    prev_ll = log_ll\n",
    "    itr += 1\n",
    "    \n",
    "#final plot\n",
    "colors = np.sum(resp[:, None, :] * rgb[:k, :, None], 0).T\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, s=3)\n",
    "plt.scatter(mus[:, 0], mus[:, 1], marker='X', edgecolors='k')\n",
    "plt.title('Step %d : log-likelihood: %.2f'%(itr, log_ll))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Singularity Issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 7813\n",
    "np.random.seed(seed)\n",
    "k = 4\n",
    "pis = np.ones(k)/(k)\n",
    "cx = np.random.uniform(low=X[:, 0].min(), high=X[:, 0].max(), size=len(pis))[:, None]\n",
    "cy = np.random.uniform(low=X[:, 1].min(), high=X[:, 1].max(), size=len(pis))[:, None]\n",
    "mus = np.hstack([cx, cy])\n",
    "Sigmas = np.asarray([np.eye(X.shape[1]) for l in pis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_ll = -np.inf\n",
    "itr = 0\n",
    "max_itr = 1000\n",
    "plot_freq = 10\n",
    "while True and itr < max_itr:\n",
    "    # E-Step\n",
    "    # get the responsibilities\n",
    "    resp, pi_N = Estep(pis, mus, Sigmas) \n",
    "    \n",
    "    # Compute log-likelihood and check for convergence\n",
    "    log_ll = np.log(1e-8 + np.sum(pi_N, 0)).sum()\n",
    "    if log_ll - prev_ll < 0.1:\n",
    "        print('Converged at step %d'%(itr))\n",
    "        break\n",
    "            \n",
    "    # Plot results\n",
    "    if itr % plot_freq == 0:\n",
    "        # Assign a color to each datapoint\n",
    "        colors = np.sum(resp[:, None, :] * rgb[:k, :, None], 0).T\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=colors, s=3)\n",
    "        plt.scatter(mus[:, 0], mus[:, 1], marker='X', edgecolors='k')\n",
    "        plt.title('Step %d : log-likelihood: %.2f'%(itr, log_ll))\n",
    "        plt.show()\n",
    "    \n",
    "    # M-Step\n",
    "    pis, mus, Sigmas = Mstep(resp, pis, mus, Sigmas)\n",
    "    \n",
    "    prev_ll = log_ll\n",
    "    itr += 1\n",
    "    \n",
    "#final plot\n",
    "colors = np.sum(resp[:, None, :] * rgb[:k, :, None], 0).T\n",
    "plt.scatter(X[:, 0], X[:, 1], c=colors, s=3)\n",
    "plt.scatter(mus[:, 0], mus[:, 1], marker='X', edgecolors='k')\n",
    "plt.title('Step %d : log-likelihood: %.2f'%(itr, log_ll))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Singularity issues can be solved by adding a prior to $\\Sigma_k$. The MLE equation for $\\Sigma_k$ is given by\n",
    "\n",
    "$$\n",
    "N_k\\Sigma_k - \\sum_{i=1}^n(x_n - \\mu_k)(x_n - \\mu_k)^\\top = 0\n",
    "$$\n",
    "\n",
    "Let's set the prior on $\\Sigma_k$ to be the Inverse Wishart. The MAP equation is now given by\n",
    "\n",
    "$$\n",
    "N_k\\Sigma_k - \\sum_{i=1}^n(x_n - \\mu_k)(x_n - \\mu_k)^\\top + \\frac{\\partial}{\\partial \\Sigma^{-1}_k}\\left[\\mathcal{W}^{-1}(\\Sigma_k; \\Psi, \\nu)\\right] = 0\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\mathcal{W}^{-1}(\\Sigma; \\Psi, \\nu) = \\frac{|\\Psi|^{\\nu/2}}{2^{\\nu p/2}\\Gamma_p(\\nu/2)}|\\Sigma|^{-(\\nu + p + 1)/2}\\exp\\left(-\\frac{1}{2}\\mathrm{tr}(\\Psi\\Sigma^{-1})\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can take the code on adding the prior on your own. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
